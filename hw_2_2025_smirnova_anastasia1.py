# -*- coding: utf-8 -*-
"""HW_2_2025_Smirnova_Anastasia1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j7cfkD4OloltMyuXF0ic8x59XXj-jsmF

# **Part1**

**1. Calculate the TF-IDF metrics for any three songs in the same language that you choose. (Task weight: 30%)**
"""

!pip install natasha nltk wordcloud matplotlib seaborn scikit-learn

import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from wordcloud import WordCloud

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.manifold import TSNE

import nltk
nltk.download("stopwords")
from nltk.corpus import stopwords
stopwords_ru = set(stopwords.words("russian"))

# natasha
from natasha import (
    Segmenter,
    MorphVocab,
    NewsEmbedding,
    NewsMorphTagger,
    Doc
)

segmenter = Segmenter()
morph_vocab = MorphVocab()
emb = NewsEmbedding()
morph_tagger = NewsMorphTagger(emb)

sns.set(style="whitegrid")

song1 = """Всего семнадцать страниц одолев, ты думал, что уже всё прочитал,
Раз и навсегда уяснив, кто в этой книжке лев, а кто шакал,
Заметок пафосных на полях немало набросал,
В духе: «Деньги — всего лишь бумага, золото — всего лишь металл».
Проблем сложней твоих, конечно, никогда никто не решал,
Ничья душа ещё так не дрожала от боли, еле дыша
И весь человеческий опыт не стоит ломаного гроша,
Ведь никто ещё так не плакал, никто ещё так не ржал.
Предки, жизни тебя уча, всего лишь ездили по ушам,
У бати вон в аттестате сплошные «тройки» — ты же видел сам.
Пора уйти, сделав разными ваши адреса,
Пожав кормившую тебя руку, которую ты так часто кусал.
Твой бунт против папы и мамы закончится офисным креслом,
Мерцающим синим экраном и нудным трудом бесполезным.
На место протеста и крика придут тишина и усталость.
Твоя жизнь — не кино и не книга, какая жалость… Какая жалость…
С этих пор ты сам себе режиссёр, сам себе актёр и сам себе оператор.
Камера! Мотор! Попёр за кадром кадр. Стоп! Снято!
В монтажной комнате памяти из этого нарежут позже,
Что-то похожее на то, что было, но не тоже самое всё же.
Самый обычный случай при просмотре вдруг проберёт до дрожи
И вставит так, что шелохнуться даже не сможешь.
Фраза, кем-то тогда небрежно брошенная, сложена
Из слов слишком правильных, даже для словаря Ожегова.
Но, по-хорошему, не стоит чересчур увлекаться перемоткой назад,
Туда, где универ, школа или детский сад,
Ведь ты уже ушёл, сделав разными ваши адреса,
Пожав кормившую тебя руку, которую так часто кусал.
Твой бунт против папы и мамы закончится офисным креслом,
Мерцающим синим экраном и нудным трудом бесполезным.
На место протеста и крика придут тишина и усталость.
Твоя жизнь — не кино и не книга, какая жалость… Какая жалость…"""
song2 = """Всё наперекocяк, безнадёжный случай
Над тобой моросят, как в мультике, тучи
И следуют по пятам, словно конвоиры
Негарантийный вариант сломанного мира
Перед тобой то ли Стикс, то ли Рубикон
То ли лабиринт из одних тупиков
Психика как мартышка с кубиком Рубика
И, конечно, ты уникум
Но я тоже бывал в положении похожем
Как и каждый, кто хоть что-то прожил
Я обречённо стоял на краю
Но ты сейчас слышишь, о чём я пою
Даже когда кругом отчаянья торжество
Когда кажется, что ты уже дна достиг
Жить есть ради чего, есть ради кого
Есть причины для радости
Даже когда кругом отчаянья торжество
Когда кажется, что ты уже дна достиг
Жить есть ради чего, есть ради кого
Есть причины для радости
Запрещённый приём — бьёшь рукой по мату
Перед глазами плывёт, дышать тяжеловато
Побеждают враги, отвернулись судьи
В кулуарах торги, провернули суки
Как быть, когда предан?
Когда мысль о доверии в будущем кажется бредом?
Когда из тебя что-то вырвали с корнем
И в яму потом ядом плюнули следом?
Да, я тоже бывал в положении похожем
Как и каждый, кто хоть что-то прожил
Я обречённо стоял на краю
Но ты сейчас слышишь, о чём я пою
Даже когда кругом отчаянья торжество
Когда кажется, что ты уже дна достиг
Жить есть ради чего, есть ради кого
Есть причины для радости
Даже когда кругом отчаянья торжество
Когда кажется, что ты уже дна достиг
Жить есть ради чего, есть ради кого
Есть причины для радости
Для печали поводов миллион
Эти пути нас вечно будут во мрак вести
Но в каждую секунду смутных времён
У тебя есть причины для радости
Даже когда кругом отчаянья торжество
Когда кажется, что ты уже дна достиг
Жить есть ради чего, есть ради кого
Есть причины для радости
Даже когда кругом отчаянья торжество
Когда кажется, что ты уже дна достиг
Жить есть ради чего, есть ради кого
Есть причины для радости"""
song3 = """Если Вселенная бесконечна действительно
Значит, есть куча копий нашей планеты
Где-то щас происходят те же события
Копия меня пишет копию песни этой
Если Вселенная бесконечна и в правду
То прямо следует из этого факта
Что ты точно так же ждёшь, когда я вернусь обратно
Где-то там, в глуби далёких галактик
То есть не ты, конечно, а твоя копия
И не меня, а мою копию, конечно
Если, конечно, всё это не утопия
И Вселенная действительно бесконечная
Действительно бесконечная
Бесконечная, бесконечная
Бесконечная, бесконечная
Бесконечная, бесконечная
Бесконечная, бесконечная
Если обдумать подробнее эту гипотезу
Посерьёзнее впариться в ситуацию
То помимо миров, повторённых полностью
Есть и такие, что могут чем-то отличаться
Получается, что где-то на задворках созвездий
Есть такие версии нашего мира
Где мы с тобой либо уже не вместе
Либо изначально прошли друг друга мимо
И мы там, может быть, даже счастливы
То есть, опять же, не мы, а они, к счастью
Эти варианты могут быть самыми разными
Но о них не хочу даже думать щас я
Бог — одинокий ребёнок
Брошенный всеми в пустом магазине игрушек
Бродя среди полок, убивающий время
Понимая, что взрослым не нужен
Он собрал уже тыщи конструкторов разных
Он распечатал всех кукол
Но он всё ещё ищет детали для пазлов
Он всё ещё ходит по кругу
А есть, наверное, и поудачней расклады
Без лишней жести, которая нам досталась
Там всё случилось не вопреки всему, а как надо
Так, как тут никогда не случалось
Мы попадём туда после смерти, может быть
Если будем добрыми и хорошими
Если играть в то, что нами с тобою прожито
Не наскучит малышу в магазине брошенному
Но, я надеюсь, это позже случится гораздо
Я туда совершенно не тороплюсь
Как бы круто там ни было, мне одно ясно
Ещё сильнее я в тебя всё равно не влюблюсь
Бог — одинокий ребёнок
Брошенный всеми в пустом магазине игрушек
Бродя среди полок, убивающий время
Понимая, что взрослым не нужен
Он собрал уже тыщи конструкторов разных
Он распечатал всех кукол
Но он всё ещё ищет детали для пазлов
Он всё ещё ходит по кругу
Бог — одинокий ребёнок
Брошенный всеми в пустом магазине игрушек
Бродя среди полок, убивающий время
Понимая, что взрослым не нужен
Он собрал уже тыщи конструкторов разных
Он распечатал всех кукол
Но он всё ещё ищет детали для пазлов
Он всё ещё ходит по кругу"""

songs = [song1, song2, song3]
titles = ["Какая жалость", "Причины для радости", "Вселенная бесконечна"]

def preprocess(text: str) -> str:

    # в нижний регистр
    text = text.lower()
    # только русские буквы и пробелы
    text = re.sub(r"[^а-яё\s]", " ", text)
    # черновая токенизация
    tokens = text.split()
    # убираем стоп-слова и слишком короткие токены
    tokens = [t for t in tokens if t not in stopwords_ru and len(t) > 2]

    # склеиваем обратно
    clean_text = " ".join(tokens)

    # лемматизация
    doc = Doc(clean_text)
    doc.segment(segmenter)
    doc.tag_morph(morph_tagger)
    for token in doc.tokens:
        token.lemmatize(morph_vocab)
    lemmas = [token.lemma for token in doc.tokens]

    return " ".join(lemmas)

songs_clean = [preprocess(s) for s in songs]

for title, text in zip(titles, songs_clean):
    print(f"=== {title} ===")
    print(text[:300], "...\n")

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(songs_clean)

df_tfidf = pd.DataFrame(
    tfidf_matrix.toarray(),
    index=titles,
    columns=tfidf.get_feature_names_out()
)

df_tfidf.head()

values = df_tfidf.values.flatten()
values = values[values > 0]  # только ненулевые

plt.figure(figsize=(8, 4))
sns.histplot(values, bins=25)
plt.title("Распределение ненулевых TF-IDF значений")
plt.xlabel("TF-IDF")
plt.ylabel("Частота")
plt.show()

plt.figure(figsize=(8, 4))
sns.boxplot(data=df_tfidf.T)
plt.title("Boxplot TF-IDF по песням Noize MC")
plt.xlabel("Песня")
plt.ylabel("TF-IDF")
plt.show()

top_words_per_song = {}

for i, title in enumerate(titles):
    row = df_tfidf.iloc[i]
    top = row.sort_values(ascending=False).head(10)
    top_words_per_song[title] = top
    print(f"\n=== Топ-10 слов по TF-IDF для: {title} ===")
    print(top)

plt.figure(figsize=(15, 5))

for i, text in enumerate(songs_clean):
    plt.subplot(1, 3, i+1)
    wc = WordCloud(
        width=600,
        height=400,
        background_color="white"
    ).generate(text)
    plt.imshow(wc)
    plt.axis("off")
    plt.title(titles[i])

plt.tight_layout()
plt.show()

vec_bigram = CountVectorizer(ngram_range=(2, 2))
X_bigrams = vec_bigram.fit_transform(songs_clean)

df_bigrams = pd.DataFrame(
    X_bigrams.toarray(),
    index=titles,
    columns=vec_bigram.get_feature_names_out()
)

bigrams_total = df_bigrams.sum().sort_values(ascending=False)

print("Топ-10 самых частых биграмм по всем песням:")
bigrams_total.head(10)

tsne = TSNE(
    n_components=2,
    random_state=42,
    perplexity=1,
    learning_rate=100
)

X_2d = tsne.fit_transform(tfidf_matrix.toarray())

df_tsne = pd.DataFrame(X_2d, columns=["x", "y"])
df_tsne["title"] = titles

plt.figure(figsize=(6, 5))
sns.scatterplot(data=df_tsne, x="x", y="y", hue="title", s=100)
plt.title("t-SNE визуализация песен в TF-IDF пространстве")
plt.legend()
plt.show()

"""### Conclusions for Part 1: TF-IDF Analysis of Three Noize MC Songs

1. **Text preprocessing** (lowercasing, stopword removal, lemmatization with Natasha) helped reduce noise in the vocabulary and made the lyrics easier to compare. After preprocessing, different word forms were unified, which improved the quality of TF-IDF analysis.

2. **TF-IDF results** show that each song has its own set of important words.  
   - *“Какая жалость”* focuses on themes of regret and personal growth (*“жалость”, “жизнь”, “твой”*).  
   - *“Причины для радости”* emphasizes struggle and motivation (*“отчаяние”, “достигнуть”, “радость”*).  
   - *“Вселенная бесконечна”* highlights existential and cosmic ideas (*“бесконечный”, “копия”, “ребёнок”*).

3. The **distribution of TF-IDF values** shows that most words have low importance, while a small group stands out with high weights. This means each song relies on its own unique set of key terms. The boxplot confirms that the TF-IDF distributions differ noticeably between songs.

4. **WordClouds** visually support the TF-IDF findings: the most important words appear prominently and differ across the three lyrics, clearly reflecting their themes.

5. The **t-SNE visualization** shows that the songs form separate clusters in TF-IDF space. Even with only three texts, the algorithm distinguishes them well, which suggests clear stylistic and thematic differences.

**Overall**, the TF-IDF analysis reveals that each Noize MC song has its own lexical and semantic profile, and preprocessing made these differences even more visible.

**2. Compare TF-IDF with other text vectorization methods, such as Count Vectorizer, Word2Vec, or Doc2Vec. (Task weight: 30%)**
"""

!pip install gensim

from sklearn.feature_extraction.text import CountVectorizer

count_vec = CountVectorizer()
count_matrix = count_vec.fit_transform(songs_clean)

df_count = pd.DataFrame(
    count_matrix.toarray(),
    index=titles,
    columns=count_vec.get_feature_names_out()
)

df_count.head()

from gensim.models import Word2Vec
tokenized = [text.split() for text in songs_clean]

w2v_model = Word2Vec(
    sentences=tokenized,
    vector_size=100,
    window=5,
    min_count=1,
    workers=4,
    epochs=200
)

def average_vector(tokens):
    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]
    if len(vectors) == 0:
        return np.zeros(w2v_model.vector_size)
    return np.mean(vectors, axis=0)

song_vectors = np.array([average_vector(tokens) for tokens in tokenized])

df_w2v = pd.DataFrame(song_vectors, index=titles)
df_w2v.head()

from sklearn.manifold import TSNE

# t-SNE для TF-IDF представлений
tsne_tfidf = TSNE(n_components=2, perplexity=1, random_state=42)
tfidf_2d = tsne_tfidf.fit_transform(tfidf_matrix.toarray())

df_tfidf_2d = pd.DataFrame(tfidf_2d, columns=["x", "y"])
df_tfidf_2d["title"] = titles

plt.figure(figsize=(6, 5))
sns.scatterplot(data=df_tfidf_2d, x="x", y="y", hue="title", s=120)
plt.title("t-SNE: songs in TF-IDF space")
plt.legend()
plt.show()

# t-SNE для Word2Vec представлений
tsne_w2v = TSNE(n_components=2, perplexity=1, random_state=42)
w2v_2d = tsne_w2v.fit_transform(song_vectors)

df_w2v_2d = pd.DataFrame(w2v_2d, columns=["x", "y"])
df_w2v_2d["title"] = titles

plt.figure(figsize=(6, 5))
sns.scatterplot(data=df_w2v_2d, x="x", y="y", hue="title", s=120)
plt.title("t-SNE: songs in Word2Vec space")
plt.legend()
plt.show()

"""### Conclusions for Part 2: Comparison of Vectorization Methods

Three vectorization methods were compared: CountVectorizer, TF-IDF, and Word2Vec.

1. **Computational complexity.**  
   CountVectorizer and TF-IDF work very fast and require almost no training.  
   Word2Vec needs iterative training, so it is more computationally expensive, although still manageable for small datasets.

2. **Representation quality.**  
   - CountVectorizer only counts word frequencies and does not capture meaning.  
   - TF-IDF highlights more important words in each text and gives a better representation.  
   - Word2Vec creates dense semantic vectors that capture relationships between words, but the resulting features are harder to interpret.

3. **Interpretability.**  
   CountVectorizer and TF-IDF are easy to interpret because each feature corresponds to a real word.  
   Word2Vec is not interpretable at the feature level because vector dimensions have no direct linguistic meaning.

**Overall**, TF-IDF gives the best balance between simplicity and usefulness for this task.  
Word2Vec is better for capturing semantics, but less suitable when clear and explainable features are required.

**3. Conduct research using the resulting transformed data.
Which words/phrases are most common and which are less common.(Task weight: 20%)**
"""

from collections import Counter

word_counts = Counter(" ".join(songs_clean).split())

top10_words = word_counts.most_common(10)
top10_words
df_top_words = pd.DataFrame(top10_words, columns=["word", "count"])
df_top_words

plt.figure(figsize=(8,5))
sns.barplot(data=df_top_words, x="count", y="word", palette="viridis")
plt.title("Top-10 Most Frequent Words Across All Songs")
plt.xlabel("Frequency")
plt.ylabel("Word")
plt.show()

vectorizer = CountVectorizer(ngram_range=(2,2))
X_bi = vectorizer.fit_transform(songs_clean)

df_bi = pd.DataFrame(
    X_bi.toarray(),
    index=titles,
    columns=vectorizer.get_feature_names_out()
)

top10_bigrams = df_bi.sum().sort_values(ascending=False).head(10)
df_top_bigrams = top10_bigrams.reset_index()
df_top_bigrams.columns = ["bigram", "count"]
df_top_bigrams

plt.figure(figsize=(10,5))
sns.barplot(data=df_top_bigrams, x="count", y="bigram", palette="magma")
plt.title("Top-10 Most Frequent Bigrams Across All Songs")
plt.xlabel("Frequency")
plt.ylabel("Bigram")
plt.show()

"""### Conclusions for Part 3: Statistical Analysis of Word and Phrase Frequencies

1. **Most frequent words.**  
   The top-10 words across all three songs show recurring themes typical for Noize MC, such as life, emotions, and personal reflection. Words like *“жизнь”, “весь”, “ещё”, “твой”* appear most often and form the basic vocabulary of the texts.

2. **Most common bigrams.**  
   The most frequent bigrams reveal stable patterns in the lyrics. Some pairs reflect emotional states (e.g., *“достигнуть дно”*) or narrative elements, showing how the songwriter builds meaning through repeated phrases.

3. **WordCloud interpretation.**  
   The WordClouds clearly highlight what each song focuses on:  
   - *“Какая жалость”* — regret and growing up,  
   - *“Причины для радости”* — struggle and motivation,  
   - *“Вселенная бесконечна”* — existential and cosmic themes.  
   These visuals support the TF-IDF findings.

4. **t-SNE visualization.**  
   The songs form separate clusters in 2D TF-IDF space, meaning their vocabularies differ noticeably. Even with only three samples, the model distinguishes them well.

5. **Overall.**  
   The combination of frequency analysis, n-grams, WordClouds, and t-SNE shows that each song has its own style and thematic direction. Although some vocabulary overlaps, each text remains distinguishable both statistically and visually.

# **Part2**
"""

!pip install transformers datasets torch scikit-learn

import torch
from torch.utils.data import Dataset, DataLoader
from torch import nn

from transformers import (
    BertTokenizer,
    BertForSequenceClassification,
    get_linear_schedule_with_warmup
)

from torch.optim import AdamW

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("IMDB Dataset.csv")

df = df[df["sentiment"].isin(["positive", "negative"])].copy()

df["label"] = df["sentiment"].map({"negative": 0, "positive": 1})
df = df[["review", "label"]]

df.head()

from sklearn.model_selection import train_test_split

train_texts, test_texts, train_labels, test_labels = train_test_split(
    df["review"].tolist(),
    df["label"].tolist(),
    test_size=0.2,
    random_state=42,
    stratify=df["label"]
)

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def encode_batch(text_list, tokenizer, max_len=256):
    return tokenizer(
        text_list,
        padding=True,
        truncation=True,
        max_length=max_len,
        return_tensors="pt"
    )

train_enc = encode_batch(train_texts, tokenizer)
test_enc  = encode_batch(test_texts, tokenizer)

class IMDBDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = IMDBDataset(train_enc, train_labels)
test_dataset  = IMDBDataset(test_enc,  test_labels)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader  = DataLoader(test_dataset,  batch_size=8)

from transformers import BertForSequenceClassification
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup

device = "cuda" if torch.cuda.is_available() else "cpu"
device

model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)
model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)

epochs = 2
total_steps = len(train_loader) * epochs

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

model.train()

for epoch in range(epochs):
    print(f"\nEpoch {epoch+1}/{epochs}")
    total_loss = 0

    for batch in train_loader:
        optimizer.zero_grad()

        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        loss = outputs.loss
        loss.backward()

        optimizer.step()
        scheduler.step()

        total_loss += loss.item()

    print(f"Average loss: {total_loss/len(train_loader):.4f}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        preds = torch.argmax(outputs.logits, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

acc  = accuracy_score(all_labels, all_preds)
prec = precision_score(all_labels, all_preds)
rec  = recall_score(all_labels, all_preds)
f1   = f1_score(all_labels, all_preds)

acc, prec, rec, f1

"""### Final Evaluation Summary

The fine-tuned BERT model (`bert-base-uncased`) achieved strong classification performance on the IMDB test set:

- **Accuracy:** 0.93  
- **Precision:** 0.93  
- **Recall:** 0.93  
- **F1-score:** 0.93  

All metrics exceed the required threshold (Accuracy ≥ 0.90), and the F1-score is almost identical to the Accuracy, indicating a well-balanced model. The classifier correctly distinguishes clearly positive and clearly negative reviews during manual inspection. Training was stable, loss decreased consistently, and inference time per review is under one second, making the model suitable for practical sentiment analysis tasks.
"""